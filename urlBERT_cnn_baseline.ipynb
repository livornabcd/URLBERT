{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KztC1mJ-M8P2"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers==4.36.2 torch==2.3.1 pandas==2.2.2 scikit-learn==1.4.2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, time, random, numpy as np, pandas as pd, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from urllib.parse import urlparse, parse_qsl, unquote, urlunparse\n"
      ],
      "metadata": {
        "id": "uSeBmlgFNB2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Mount & paths -----------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "BASE = \"/content/drive/My Drive/DLI Assignment\"\n",
        "TRAIN_CSV, TEST_CSV = f\"{BASE}/Train.csv\", f\"{BASE}/Test.csv\"\n",
        "VOCAB_TXT, URLBERT_PT = f\"{BASE}/vocab.txt\", f\"{BASE}/urlBERT.pt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHQ84-rMNGtd",
        "outputId": "799e2a29-e30b-442a-c049-87db234eb7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Repro / Device -----------------\n",
        "SEED=42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "import torch, torch.nn as nn\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
      ],
      "metadata": {
        "id": "gjJx33weNI6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Default knobs (will be shrunk by time budget) -----------------\n",
        "VAL_SIZE = 0.20\n",
        "CAP_TRAIN_TOTAL   = None\n",
        "CAP_VAL_TOTAL     = 8000\n",
        "CAP_TEST_TOTAL    = None\n",
        "\n",
        "# Stage-1 head train\n",
        "TIME_BUDGET_STAGE1 = 15     # will drop if time is tight\n",
        "BATCH_TRAIN        = 96\n",
        "MAX_LEN_TRAIN      = 80\n",
        "MAX_LEN_INFER      = 96\n",
        "\n",
        "# Uncertainty BERT (strict Top-K; dynamically reduced)\n",
        "MAX_BERT_VAL_BASE  = 4000\n",
        "MAX_BERT_TEST_BASE = 5000\n",
        "BATCH_INFER_INIT   = 4096\n",
        "BATCH_INFER_MIN    = 256\n",
        "VAL_IMPROVE_MARGIN = 0.0002  # require real gain from BERT before using it\n",
        "\n",
        "# Lexical ensemble\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "CAP_TRAIN_LEX_BASE = 20000\n",
        "N_FEATURES         = 2**20\n",
        "MAX_ITER_SGD_BASE  = 12\n",
        "CHAR_CFGS          = [(3,6),(4,7)]\n",
        "TOK_CFGS           = [(1,3),(2,4)]\n",
        "\n",
        "# ACC overrides & buckets\n",
        "MIN_COUNT_LIST = [1,2,3,5,8,10,20]\n",
        "P_HI_LIST      = [0.93,0.95,0.97,0.99,0.995]\n",
        "P_LO_LIST      = [0.005,0.01,0.02,0.03,0.05]\n",
        "HOST_BUCKETS   = [(0,0),(1,2),(3,5),(6,20),(21,100),(101,1000),(1001,9999999)]\n",
        "DOM_BUCKETS    = [(0,0),(1,2),(3,5),(6,20),(21,100),(101,1000),(1001,9999999)]\n",
        "TLD_BUCKETS    = [(0,0),(1,2),(3,10),(11,50),(51,9999999)]\n"
      ],
      "metadata": {
        "id": "O2LIfKMRNMlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Helpers -----------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, recall_score, f1_score\n",
        "\n",
        "def load_csv(path):\n",
        "    df = pd.read_csv(path)\n",
        "    url_col = next((c for c in df.columns if re.search(r\"\\burl\\b\", str(c), re.I)), df.columns[0])\n",
        "    y_col = None\n",
        "    for k in [\"label\",\"class\",\"target\",\"y\",\"is_phish\",\"phishing\",\"malicious\"]:\n",
        "        m = [c for c in df.columns if re.search(rf\"\\b{k}\\b\", str(c), re.I)]\n",
        "        if m: y_col = m[0]; break\n",
        "    if y_col is None: y_col = df.columns[1]\n",
        "    df = df[[url_col, y_col]].copy(); df.columns = [\"url\",\"raw\"]\n",
        "    def norm(v):\n",
        "        s=str(v).strip().lower()\n",
        "        if s in {\"1\",\"true\",\"phish\",\"phishing\",\"malicious\",\"bad\",\"attack\",\"fraud\",\"spam\",\"harm\"}: return 1\n",
        "        if s in {\"0\",\"false\",\"benign\",\"legit\",\"legitimate\",\"good\",\"safe\",\"normal\",\"clean\"}: return 0\n",
        "        try: return 1 if float(s)>=0.5 else 0\n",
        "        except: return None\n",
        "    df[\"label\"]=df[\"raw\"].map(norm); df=df.dropna(subset=[\"url\",\"label\"])\n",
        "    df[\"label\"]=df[\"label\"].astype(int); df[\"url\"]=df[\"url\"].astype(str)\n",
        "    return df[[\"url\",\"label\"]]\n",
        "\n",
        "def cap_total_stratified(df, cap):\n",
        "    if cap is None or len(df)<=cap: return df.reset_index(drop=True)\n",
        "    dist = df[\"label\"].value_counts(normalize=True)\n",
        "    quotas = (dist*cap).round().astype(int).to_dict()\n",
        "    parts=[]\n",
        "    for c,k in quotas.items():\n",
        "        pool = df[df[\"label\"]==c]\n",
        "        parts.append(pool.sample(min(k,len(pool)), random_state=SEED))\n",
        "    return pd.concat(parts).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# Canonicalizers for priors\n",
        "def host_only(u):\n",
        "    try:\n",
        "        h=urlparse(u).netloc.lower()\n",
        "        if \"@\" in h: h=h.split(\"@\",1)[1]\n",
        "        if \":\" in h: h=h.split(\":\",1)[0]\n",
        "        if h.startswith(\"www.\"): h=h[4:]\n",
        "        return h\n",
        "    except: return \"\"\n",
        "def registrable_domain(u):\n",
        "    h=host_only(u); parts=[p for p in h.split(\".\") if p]\n",
        "    return \".\".join(parts[-2:]) if len(parts)>=2 else h\n",
        "def tld(u):\n",
        "    h=host_only(u); return h.split(\".\")[-1] if \".\" in h else \"\"\n",
        "def path_first(u):\n",
        "    try:\n",
        "        p=urlparse(u).path; seg=[s for s in p.split(\"/\") if s]; return seg[0] if seg else \"\"\n",
        "    except: return \"\"\n",
        "def path_last(u):\n",
        "    try:\n",
        "        p=urlparse(u).path; seg=[s for s in p.split(\"/\") if s]; return seg[-1] if seg else \"\"\n",
        "    except: return \"\"\n",
        "def query_keys(u):\n",
        "    try:\n",
        "        q=dict(parse_qsl(urlparse(u).query, keep_blank_values=True)); return \"&\".join(sorted(q.keys()))\n",
        "    except: return \"\"\n",
        "def normalize_url(u: str) -> str:\n",
        "    try:\n",
        "        p=urlparse(u)\n",
        "        net=p.netloc.lower()\n",
        "        if net.startswith(\"www.\"): net=net[4:]\n",
        "        if net.endswith(\":80\"): net=net[:-3]\n",
        "        if net.endswith(\":443\"): net=net[:-4]\n",
        "        path = unquote(p.path)\n",
        "        path = re.sub(r\"/{2,}\", \"/\", path)\n",
        "        if len(path)>1 and path.endswith(\"/\"): path=path[:-1]\n",
        "        return urlunparse((p.scheme, net, path, p.params, p.query, p.fragment))\n",
        "    except: return u\n",
        "\n",
        "def best_acc_threshold(y_true, scores):\n",
        "    y_true=np.asarray(y_true); scores=np.asarray(scores)\n",
        "    P=int(y_true.sum()); N=y_true.size-P\n",
        "    if y_true.size==0: return 0.5, float(\"nan\")\n",
        "    if P==0: return 1.0, 1.0\n",
        "    if N==0: return 0.0, 1.0\n",
        "    fpr,tpr,thr = roc_curve(y_true, scores)\n",
        "    accs=(tpr*P + (1-fpr)*N)/(P+N)\n",
        "    i=int(np.argmax(accs))\n",
        "    return float(thr[i]), float(accs[i])\n",
        "\n",
        "def make_prior(series, labels):\n",
        "    grp = pd.DataFrame({\"k\":series, \"y\":labels}).groupby(\"k\")[\"y\"].agg([\"sum\",\"count\"]).reset_index()\n",
        "    grp[\"rate\"] = (grp[\"sum\"]+1)/(grp[\"count\"]+2)\n",
        "    return dict(zip(grp[\"k\"], grp[\"rate\"])), dict(zip(grp[\"k\"], grp[\"count\"]))\n",
        "\n",
        "def apply_overrides(scores,\n",
        "                    cnt_host, rate_host, cnt_dom, rate_dom, cnt_tld, rate_tld,\n",
        "                    cnt_url, rate_url, cnt_pf, rate_pf, cnt_pl, rate_pl, cnt_qk, rate_qk,\n",
        "                    cnt_h_pf, rate_h_pf, cnt_d_pf, rate_d_pf, cnt_d_pl, rate_d_pl,\n",
        "                    minc, plo, phi, hi=0.99995, lo=0.00005):\n",
        "    out=scores.copy()\n",
        "    # Priority: URL > host+pf > dom+pf > dom+pl > host > dom > tld > pf > pl > qk\n",
        "    M = (cnt_url>=max(1,minc)) & (rate_url>=phi); out[M]=hi\n",
        "    M = (cnt_url>=max(1,minc)) & (rate_url<=plo); out[M]=lo\n",
        "    M = (cnt_h_pf>=minc) & (rate_h_pf>=phi); out[M]=hi\n",
        "    M = (cnt_h_pf>=minc) & (rate_h_pf<=plo); out[M]=lo\n",
        "    M = (cnt_d_pf>=minc) & (rate_d_pf>=phi); out[M]=hi\n",
        "    M = (cnt_d_pf>=minc) & (rate_d_pf<=plo); out[M]=lo\n",
        "    M = (cnt_d_pl>=minc) & (rate_d_pl>=phi); out[M]=hi\n",
        "    M = (cnt_d_pl>=minc) & (rate_d_pl<=plo); out[M]=lo\n",
        "    M = (cnt_host>=minc) & (rate_host>=phi); out[M]=hi\n",
        "    M = (cnt_host>=minc) & (rate_host<=plo); out[M]=lo\n",
        "    M = (cnt_dom>=minc) & (rate_dom>=phi); out[M]=hi\n",
        "    M = (cnt_dom>=minc) & (rate_dom<=plo); out[M]=lo\n",
        "    M = (cnt_tld>=minc) & (rate_tld>=phi); out[M]=hi\n",
        "    M = (cnt_tld>=minc) & (rate_tld<=plo); out[M]=lo\n",
        "    M = (cnt_pf>=minc) & (rate_pf>=phi); out[M]=hi\n",
        "    M = (cnt_pf>=minc) & (rate_pf<=plo); out[M]=lo\n",
        "    M = (cnt_pl>=minc) & (rate_pl>=phi); out[M]=hi\n",
        "    M = (cnt_pl>=minc) & (rate_pl<=plo); out[M]=lo\n",
        "    M = (cnt_qk>=minc) & (rate_qk>=phi); out[M]=hi\n",
        "    M = (cnt_qk>=minc) & (rate_qk<=plo); out[M]=lo\n",
        "    return out\n",
        "\n",
        "def bucketize(cnt, BUCKS):\n",
        "    for i,(lo,hi) in enumerate(BUCKS):\n",
        "        if lo <= cnt <= hi: return i\n",
        "    return len(BUCKS)-1\n",
        "def bucketize_vec(cnts, bucks):\n",
        "    return np.array([bucketize(int(c), bucks) for c in cnts], dtype=np.int32)\n"
      ],
      "metadata": {
        "id": "5B-gI2IANRd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Load & split -----------------\n",
        "train_full = load_csv(TRAIN_CSV); test_full = load_csv(TEST_CSV)\n",
        "train_df, val_df = train_test_split(train_full, test_size=VAL_SIZE, random_state=SEED, stratify=train_full[\"label\"])\n",
        "train_df = cap_total_stratified(train_df, CAP_TRAIN_TOTAL)\n",
        "val_df   = cap_total_stratified(val_df,   CAP_VAL_TOTAL)\n",
        "test_df  = cap_total_stratified(test_full, CAP_TEST_TOTAL)\n",
        "\n",
        "# add fields\n",
        "for df in (train_df, val_df, test_df):\n",
        "    df[\"_url_raw\"] = df[\"url\"]\n",
        "    df[\"url\"]   = df[\"url\"].map(normalize_url)\n",
        "    df[\"_url\"]  = df[\"url\"]\n",
        "    df[\"_host\"] = df[\"url\"].map(host_only)\n",
        "    df[\"_dom\"]  = df[\"url\"].map(registrable_domain)\n",
        "    df[\"_tld\"]  = df[\"url\"].map(tld)\n",
        "    df[\"_pf\"]   = df[\"url\"].map(path_first)\n",
        "    df[\"_pl\"]   = df[\"url\"].map(path_last)\n",
        "    df[\"_qk\"]   = df[\"url\"].map(query_keys)\n",
        "    df[\"_h_pf\"] = df[\"_host\"] + \"||\" + df[\"_pf\"]\n",
        "    df[\"_d_pf\"] = df[\"_dom\"]  + \"||\" + df[\"_pf\"]\n",
        "    df[\"_d_pl\"] = df[\"_dom\"]  + \"||\" + df[\"_pl\"]\n"
      ],
      "metadata": {
        "id": "S5MsaZjHNUi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- urlBERT-CNN (unchanged main model) -----------------\n",
        "from transformers import BertTokenizerFast, BertModel, BertConfig\n",
        "tokenizer = BertTokenizerFast(vocab_file=VOCAB_TXT, do_lower_case=False)\n",
        "cfg = BertConfig(vocab_size=tokenizer.vocab_size, hidden_size=768,\n",
        "                 num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,\n",
        "                 max_position_embeddings=512, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1)\n",
        "encoder = BertModel(cfg)\n",
        "try:\n",
        "    sd = torch.load(URLBERT_PT, map_location=\"cpu\")\n",
        "    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n",
        "    encoder.load_state_dict(sd, strict=False)\n",
        "except: pass\n",
        "encoder.config.output_hidden_states = True\n",
        "encoder.to(device)\n",
        "\n",
        "class CNNHead(nn.Module):\n",
        "    def __init__(self, hidden=768, out_ch=256, ks=(2,3,4,5), dropout=0.10):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(hidden, out_ch, k) for k in ks])\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "        self.fc    = nn.Linear(out_ch*len(ks), 2)\n",
        "        for m in self.convs: nn.init.kaiming_uniform_(m.weight, a=0.1)\n",
        "        nn.init.xavier_uniform_(self.fc.weight); nn.init.zeros_(self.fc.bias)\n",
        "    def forward(self, tok):\n",
        "        x = tok.transpose(1,2)\n",
        "        feats = [torch.max(torch.relu(conv(x)), dim=-1).values for conv in self.convs]\n",
        "        z = torch.cat(feats, dim=1)\n",
        "        return self.fc(self.drop(z))\n",
        "\n",
        "class URLBERT_CNN(nn.Module):\n",
        "    def __init__(self, enc):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.cnn = CNNHead(hidden=enc.config.hidden_size)\n",
        "        self.cls = nn.Linear(enc.config.hidden_size, 2)\n",
        "        nn.init.xavier_uniform_(self.cls.weight); nn.init.zeros_(self.cls.bias)\n",
        "        self.blend_logit = nn.Parameter(torch.tensor(0.0))\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.enc(input_ids=input_ids, attention_mask=attention_mask, return_dict=True, output_hidden_states=True)\n",
        "        last4 = torch.stack(out.hidden_states[-4:], dim=0).mean(0)\n",
        "        cls_rep = last4[:,0,:]\n",
        "        logits_cnn = self.cnn(last4)\n",
        "        logits_cls = self.cls(cls_rep)\n",
        "        w = torch.sigmoid(self.blend_logit)\n",
        "        return w*logits_cnn + (1-w)*logits_cls\n",
        "\n",
        "model = URLBERT_CNN(encoder).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def pretokenize(df, max_len=MAX_LEN_TRAIN, chunk=20000):\n",
        "    urls=df[\"url\"].tolist(); input_ids=[]; attn=[]\n",
        "    for i in range(0,len(urls),chunk):\n",
        "        enc = tokenizer(urls[i:i+chunk], truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n",
        "        input_ids.append(enc[\"input_ids\"]); attn.append(enc[\"attention_mask\"])\n",
        "    input_ids=torch.cat(input_ids,0); attn=torch.cat(attn,0)\n",
        "    y=torch.tensor(df[\"label\"].astype(int).values, dtype=torch.long)\n",
        "    return TensorDataset(input_ids, attn, y)\n",
        "\n",
        "# shrink Stage-1 if time is already low\n",
        "if remaining() < 200: TIME_BUDGET_STAGE1 = max(10, int(remaining()*0.1))\n",
        "if remaining() < 200: MAX_LEN_TRAIN = min(MAX_LEN_TRAIN, 80)\n",
        "\n",
        "train_ds = pretokenize(train_df, MAX_LEN_TRAIN)\n",
        "pin = torch.cuda.is_available()\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True, num_workers=0, pin_memory=pin)\n",
        "\n",
        "for p in model.enc.parameters(): p.requires_grad=False\n",
        "model.enc.eval(); model.cnn.train(); model.cls.train()\n",
        "opt = torch.optim.AdamW([p for n,p in model.named_parameters() if not n.startswith(\"enc.\")], lr=1e-3, weight_decay=1e-4)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "def forward_frozen(self, input_ids, attention_mask):\n",
        "    with torch.no_grad():\n",
        "        out = self.enc(input_ids=input_ids, attention_mask=attention_mask, return_dict=True, output_hidden_states=True)\n",
        "        last4 = torch.stack(out.hidden_states[-4:], dim=0).mean(0)\n",
        "        cls_rep = last4[:,0,:]\n",
        "    logits_cnn = self.cnn(last4); logits_cls = self.cls(cls_rep)\n",
        "    w = torch.sigmoid(self.blend_logit)\n",
        "    return w*logits_cnn + (1-w)*logits_cls\n",
        "\n",
        "import types; model.forward_frozen = types.MethodType(forward_frozen, model)\n",
        "\n",
        "deadline = time.time() + TIME_BUDGET_STAGE1\n",
        "batches = 0\n",
        "while time.time() < deadline:\n",
        "    for ids,attn,y in train_loader:\n",
        "        if time.time() >= deadline: break\n",
        "        ids,attn,y = ids.to(device), attn.to(device), y.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model.forward_frozen(ids,attn); loss = criterion(logits,y)\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt); scaler.update()\n",
        "        batches += 1\n",
        "        if batches >= 120: break\n",
        "    if batches >= 120: break\n"
      ],
      "metadata": {
        "id": "p524VenWNYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Priors (incl. composites) -----------------\n",
        "def prior_tables(df):\n",
        "    pri={}\n",
        "    for key in [\"_url\",\"_host\",\"_dom\",\"_tld\",\"_pf\",\"_pl\",\"_qk\",\"_h_pf\",\"_d_pf\",\"_d_pl\"]:\n",
        "        pri[f\"rate{key}\"], pri[f\"cnt{key}\"] = make_prior(df[key], df[\"label\"])\n",
        "    pri[\"global_rate\"] = (df[\"label\"].sum()+1)/(len(df)+2)\n",
        "    return pri\n",
        "pri = prior_tables(train_df)\n",
        "\n",
        "def vec_maps(df, pri):\n",
        "    gr = pri[\"global_rate\"]\n",
        "    def mapv(tag):\n",
        "        rmap, cmap = pri[f\"rate{tag}\"], pri[f\"cnt{tag}\"]\n",
        "        vals = df[tag].values\n",
        "        r = np.array([rmap.get(v,gr) for v in vals], dtype=np.float32)\n",
        "        c = np.array([cmap.get(v,0)   for v in vals], dtype=np.int32)\n",
        "        return r,c\n",
        "    r_url,c_url = mapv(\"_url\");  r_host,c_host=mapv(\"_host\"); r_dom,c_dom=mapv(\"_dom\"); r_tld,c_tld=mapv(\"_tld\")\n",
        "    r_pf,c_pf   = mapv(\"_pf\");   r_pl,c_pl   = mapv(\"_pl\");   r_qk,c_qk   = mapv(\"_qk\")\n",
        "    r_hpf,c_hpf = mapv(\"_h_pf\"); r_dpf,c_dpf = mapv(\"_d_pf\"); r_dpl,c_dpl = mapv(\"_d_pl\")\n",
        "    return (r_url,c_url, r_host,c_host, r_dom,c_dom, r_tld,c_tld, r_pf,c_pf, r_pl,c_pl, r_qk,c_qk, r_hpf,c_hpf, r_dpf,c_dpf, r_dpl,c_dpl)\n"
      ],
      "metadata": {
        "id": "rx8ju6h5Nb_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Lexical + Hinge (shared char transforms) -----------------\n",
        "# dynamically shrink lexical cap & iters if time is tight\n",
        "CAP_TRAIN_LEX = CAP_TRAIN_LEX_BASE if remaining() > 160 else int(CAP_TRAIN_LEX_BASE*0.6)\n",
        "MAX_ITER_SGD  = MAX_ITER_SGD_BASE if remaining() > 160 else max(8, int(MAX_ITER_SGD_BASE*0.75))\n",
        "\n",
        "lex_train = train_df.sample(min(CAP_TRAIN_LEX, len(train_df)), random_state=SEED)\n",
        "urls_tr = lex_train[\"url\"].tolist(); y_tr = lex_train[\"label\"].values\n",
        "urls_va = val_df[\"url\"].tolist();    y_va = val_df[\"label\"].values\n",
        "urls_te = test_df[\"url\"].tolist()\n",
        "\n",
        "def token_analyzer(s): return re.split(r'[/\\.\\?\\&=\\-_:%\\d]+', s)\n",
        "\n",
        "# Char vectorizers (shared)\n",
        "char_vecs = []\n",
        "for rng in CHAR_CFGS:\n",
        "    v = HashingVectorizer(analyzer=\"char\", ngram_range=rng, n_features=N_FEATURES,\n",
        "                          alternate_sign=False, norm=\"l2\")\n",
        "    char_vecs.append(v)\n",
        "# Fit both log-loss and hinge on SAME X\n",
        "char_log_clfs, char_hinge_clfs = [], []\n",
        "for v in char_vecs:\n",
        "    Xtr = v.transform(urls_tr); Xva = v.transform(urls_va); Xte = v.transform(urls_te)\n",
        "    clf_log   = SGDClassifier(loss=\"log_loss\", alpha=1e-5, max_iter=MAX_ITER_SGD, tol=1e-3,\n",
        "                              penalty=\"l2\", random_state=SEED).fit(Xtr,y_tr)\n",
        "    clf_hinge = SGDClassifier(loss=\"hinge\",    alpha=5e-6, max_iter=8,              tol=1e-3,\n",
        "                              penalty=\"l2\", random_state=SEED).fit(Xtr,y_tr)\n",
        "    v._Xva = Xva; v._Xte = Xte\n",
        "    char_log_clfs.append(clf_log)\n",
        "    char_hinge_clfs.append(clf_hinge)\n",
        "\n",
        "# Token vectorizers (log-loss only)\n",
        "tok_vecs, tok_log_clfs = [], []\n",
        "for rng in TOK_CFGS:\n",
        "    v = HashingVectorizer(analyzer=token_analyzer, ngram_range=rng, n_features=N_FEATURES,\n",
        "                          alternate_sign=False, norm=\"l2\")\n",
        "    Xtr = v.transform(urls_tr); Xva = v.transform(urls_va); Xte = v.transform(urls_te)\n",
        "    clf = SGDClassifier(loss=\"log_loss\", alpha=2e-5, max_iter=MAX_ITER_SGD, tol=1e-3,\n",
        "                        penalty=\"l2\", random_state=SEED).fit(Xtr,y_tr)\n",
        "    v._Xva = Xva; v._Xte = Xte\n",
        "    tok_vecs.append(v); tok_log_clfs.append(clf)\n",
        "\n",
        "def lex_and_hinge_scores(pre_va=True):\n",
        "    ps_lex, ps_hinge = [], []\n",
        "    for v,clf in zip(char_vecs, char_log_clfs):\n",
        "        X = v._Xva if pre_va else v._Xte\n",
        "        ps_lex.append(clf.predict_proba(X)[:,1])\n",
        "    for v,clf in zip(char_vecs, char_hinge_clfs):\n",
        "        X = v._Xva if pre_va else v._Xte\n",
        "        ps_hinge.append(1/(1+np.exp(-clf.decision_function(X))))\n",
        "    for v,clf in zip(tok_vecs, tok_log_clfs):\n",
        "        X = v._Xva if pre_va else v._Xte\n",
        "        ps_lex.append(clf.predict_proba(X)[:,1])\n",
        "    p_lex   = np.mean(np.stack(ps_lex,   0), axis=0).astype(np.float32)\n",
        "    p_hinge = np.mean(np.stack(ps_hinge, 0), axis=0).astype(np.float32)\n",
        "    return p_lex, p_hinge\n",
        "\n",
        "p_lex_val,  p_hinge_val  = lex_and_hinge_scores(pre_va=True)\n",
        "p_lex_test, p_hinge_test = lex_and_hinge_scores(pre_va=False)\n"
      ],
      "metadata": {
        "id": "63sjOcraNe_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Prior blend (grid on VAL) -----------------\n",
        "(r_url_v,c_url_v, r_host_v,c_host_v, r_dom_v,c_dom_v, r_tld_v,c_tld_v,\n",
        " r_pf_v,c_pf_v, r_pl_v,c_pl_v, r_qk_v,c_qk_v, r_hpf_v,c_hpf_v, r_dpf_v,c_dpf_v, r_dpl_v,c_dpl_v) = vec_maps(val_df, pri)\n",
        "(r_url_t,c_url_t, r_host_t,c_host_t, r_dom_t,c_dom_t, r_tld_t,c_tld_t,\n",
        " r_pf_t,c_pf_t, r_pl_t,c_pl_t, r_qk_t,c_qk_t, r_hpf_t,c_hpf_t, r_dpf_t,c_dpf_t, r_dpl_t,c_dpl_t) = vec_maps(test_df, pri)\n",
        "\n",
        "def blend_priors(vals_tuple, weights):\n",
        "    R = np.vstack(vals_tuple).T\n",
        "    w = np.array(weights, dtype=np.float32); sw = w.sum()\n",
        "    return (R @ w)/sw\n",
        "\n",
        "PRIOR_WEIGHTS_GRID = [\n",
        "    (6,10,12,2,3,3,1, 8,6,4),\n",
        "    (5, 9,12,2,2,2,1, 7,6,4),\n",
        "    (4, 8,10,2,2,2,1, 7,6,4),\n",
        "    (3, 7,10,1,1,1,1, 6,5,3),\n",
        "]\n",
        "best = {\"acc\":-1}\n",
        "for w in PRIOR_WEIGHTS_GRID:\n",
        "    ppri_v = blend_priors((r_url_v,r_host_v,r_dom_v,r_tld_v,r_pf_v,r_pl_v,r_qk_v, r_hpf_v,r_dpf_v,r_dpl_v), w)\n",
        "    for wl in np.linspace(0.0,1.0,11):\n",
        "        pv = wl*p_lex_val + (1.0-wl)*ppri_v\n",
        "        thr, acc = best_acc_threshold(y_va, pv)\n",
        "        if acc>best[\"acc\"]: best={\"acc\":acc,\"thr\":thr,\"wl\":float(wl),\"w\":w}\n",
        "\n",
        "ppri_t = blend_priors((r_url_t,r_host_t,r_dom_t,r_tld_t,r_pf_t,r_pl_t,r_qk_t, r_hpf_t,r_dpf_t,r_dpl_t), best[\"w\"])\n",
        "p_base_val  = best[\"wl\"]*p_lex_val  + (1.0-best[\"wl\"])*blend_priors((r_url_v,r_host_v,r_dom_v,r_tld_v,r_pf_v,r_pl_v,r_qk_v, r_hpf_v,r_dpf_v,r_dpl_v), best[\"w\"])\n",
        "p_base_test = best[\"wl\"]*p_lex_test + (1.0-best[\"wl\"])*ppri_t\n",
        "thr0, acc0 = best[\"thr\"], best[\"acc\"]\n"
      ],
      "metadata": {
        "id": "U9-5MmQXNhuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- urlBERT on strict Top-K uncertain (dynamically reduced) -----------------\n",
        "def select_topk_uncertain(p, max_n):\n",
        "    if max_n <= 0: return np.array([], dtype=np.int64)\n",
        "    return np.argsort(np.abs(p-0.5))[:max_n]\n",
        "\n",
        "def bert_probs_fast(urls, init_bs=BATCH_INFER_INIT, min_bs=BATCH_INFER_MIN, max_len=MAX_LEN_INFER):\n",
        "    model.eval()\n",
        "    out = np.empty(len(urls), dtype=np.float32); i, bs = 0, init_bs\n",
        "    while i < len(urls):\n",
        "        try:\n",
        "            j = min(i+bs, len(urls))\n",
        "            enc = tokenizer(urls[i:j], truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
        "            ids=enc[\"input_ids\"].to(device, non_blocking=True)\n",
        "            attn=enc[\"attention_mask\"].to(device, non_blocking=True)\n",
        "            with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                p = torch.softmax(model(ids,attn),1)[:,1].float().cpu().numpy()\n",
        "            out[i:j]=p; i=j\n",
        "        except RuntimeError as e:\n",
        "            if \"CUDA out of memory\" in str(e) and bs>min_bs:\n",
        "                torch.cuda.empty_cache(); bs//=2\n",
        "            else: raise\n",
        "    return out\n",
        "\n",
        "# shrink BERT work if time is tight\n",
        "def pick_k(base_k):\n",
        "    r = remaining()\n",
        "    if r < 70:  return max(1000, base_k//4)\n",
        "    if r < 140: return max(2000, base_k//2)\n",
        "    return base_k\n",
        "\n",
        "MAX_BERT_VAL  = pick_k(MAX_BERT_VAL_BASE)\n",
        "MAX_BERT_TEST = pick_k(MAX_BERT_TEST_BASE)\n",
        "if remaining() < 200:\n",
        "    MAX_LEN_INFER = min(MAX_LEN_INFER, 80)\n",
        "\n",
        "idx_v = select_topk_uncertain(p_base_val,  MAX_BERT_VAL)\n",
        "p_val_try = p_base_val.copy()\n",
        "if idx_v.size:\n",
        "    p_val_try[idx_v] = bert_probs_fast([val_df[\"url\"].tolist()[i] for i in idx_v])\n",
        "thr1, acc1 = best_acc_threshold(y_va, p_val_try)\n",
        "use_bert = (acc1 - acc0) >= VAL_IMPROVE_MARGIN\n",
        "\n",
        "if use_bert and remaining()>40:   # ensure we have time left for test BERT\n",
        "    p_val = p_val_try\n",
        "    idx_t = select_topk_uncertain(p_base_test, MAX_BERT_TEST)\n",
        "    p_test = p_base_test.copy()\n",
        "    if idx_t.size:\n",
        "        p_test[idx_t] = bert_probs_fast([test_df[\"url\"].tolist()[i] for i in idx_t])\n",
        "else:\n",
        "    p_val, p_test = p_base_val, p_base_test"
      ],
      "metadata": {
        "id": "cKH9iZG-NlIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Engineered features + HGB meta (iterations shrink if needed) -----------------\n",
        "def featurize(df):\n",
        "    u = df[\"url\"].values\n",
        "    h = df[\"_host\"].values\n",
        "    p = np.array([urlparse(x).path for x in u], dtype=object)\n",
        "    q = np.array([urlparse(x).query for x in u], dtype=object)\n",
        "    Lu = np.vectorize(len)(u); Lh=np.vectorize(len)(h)\n",
        "    Lp = np.vectorize(len)(p); Lq=np.vectorize(len)(q)\n",
        "    dots   = np.array([x.count(\".\") for x in h])\n",
        "    hyph   = np.array([x.count(\"-\") for x in u])\n",
        "    slsh   = np.array([x.count(\"/\") for x in u])\n",
        "    digs   = np.array([sum(c.isdigit() for c in u[i]) for i in range(len(u))])\n",
        "    pct    = np.array([u[i].count(\"%\") for i in range(len(u))])\n",
        "    depth  = np.array([max(0, len([s for s in urlparse(x).path.split(\"/\") if s])) for x in u])\n",
        "    qn     = np.array([len(parse_qsl(urlparse(x).query, keep_blank_values=True)) for x in u])\n",
        "    dfrac  = (digs / np.maximum(1, Lu))\n",
        "    t = df[\"_tld\"].values\n",
        "    is_cc  = np.array([1 if len(str(tt))==2 else 0 for tt in t])\n",
        "    t_is_com = np.array([1 if str(tt)==\"com\" else 0 for tt in t])\n",
        "    kw_list = [\"login\",\"verify\",\"account\",\"update\",\"secure\",\"confirm\",\"pay\",\"bank\",\"free\",\"click\",\"signin\",\"reset\",\"gift\",\"offer\"]\n",
        "    kw_feat = np.array([[1 if k in u[i].lower() else 0 for k in kw_list] for i in range(len(u))], dtype=np.int16)\n",
        "    X = np.column_stack([Lu,Lh,Lp,Lq,dots,hyph,slsh,digs,pct,depth,qn,dfrac,is_cc,t_is_com, kw_feat])\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "X_tr = featurize(train_df); y_tr = train_df[\"label\"].values\n",
        "X_va = featurize(val_df);   y_va = val_df[\"label\"].values\n",
        "X_te = featurize(test_df)\n",
        "\n",
        "HGB_ITERS = 200 if remaining() > 120 else 120\n",
        "meta_ITERS= 160 if remaining() > 90  else 120\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(max_depth=6, max_iter=HGB_ITERS, learning_rate=0.1,\n",
        "                                     validation_fraction=None, random_state=SEED)\n",
        "hgb.fit(X_tr, y_tr)\n",
        "p_hgb_val = hgb.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "p_hgb_test= hgb.predict_proba(X_te)[:,1].astype(np.float32)\n"
      ],
      "metadata": {
        "id": "QhfG8vloNn_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Meta stacker (HGB) over scores + cheap priors -----------------\n",
        "gr = pri[\"global_rate\"]\n",
        "def mapv(df, tag):\n",
        "    rmap, cmap = pri[f\"rate{tag}\"], pri[f\"cnt{tag}\"]\n",
        "    vals = df[tag].values\n",
        "    r = np.array([rmap.get(v,gr) for v in vals], dtype=np.float32)\n",
        "    c = np.array([cmap.get(v,0)   for v in vals], dtype=np.int32)\n",
        "    return r, np.clip(c, 0, 1000).astype(np.int32)\n",
        "\n",
        "r_host_v2,c_host_v2 = mapv(val_df, \"_host\");   r_dom_v2,c_dom_v2 = mapv(val_df, \"_dom\")\n",
        "r_tld_v2,c_tld_v2   = mapv(val_df, \"_tld\");    r_pf_v2, c_pf_v2  = mapv(val_df, \"_pf\")\n",
        "r_host_t2,c_host_t2 = mapv(test_df, \"_host\");  r_dom_t2,c_dom_t2 = mapv(test_df, \"_dom\")\n",
        "r_tld_t2,c_tld_t2   = mapv(test_df, \"_tld\");   r_pf_t2, c_pf_t2  = mapv(test_df, \"_pf\")\n",
        "\n",
        "def depth_vec(df):\n",
        "    return np.array([max(0, len([s for s in urlparse(u).path.split('/') if s])) for u in df[\"url\"].values], dtype=np.int16)\n",
        "\n",
        "depth_v = depth_vec(val_df); depth_t = depth_vec(test_df)\n",
        "\n",
        "S_val = np.column_stack([\n",
        "    p_val,                       # urlBERT-gated\n",
        "    p_lex_val,                   # lexical\n",
        "    p_base_val,                  # prior-blend ref\n",
        "    p_hgb_val,                   # engineered HGB\n",
        "    p_hinge_val,                 # hinge char\n",
        "    r_host_v2, r_dom_v2, r_tld_v2,\n",
        "    c_host_v2, c_dom_v2, c_tld_v2,\n",
        "    depth_v\n",
        "]).astype(np.float32)\n",
        "\n",
        "S_tst = np.column_stack([\n",
        "    p_test,\n",
        "    p_lex_test,\n",
        "    p_base_test,\n",
        "    p_hgb_test,\n",
        "    p_hinge_test,\n",
        "    r_host_t2, r_dom_t2, r_tld_t2,\n",
        "    c_host_t2, c_dom_t2, c_tld_t2,\n",
        "    depth_t\n",
        "]).astype(np.float32)\n",
        "\n",
        "meta = HistGradientBoostingClassifier(max_depth=4, max_iter=meta_ITERS, learning_rate=0.12,\n",
        "                                      validation_fraction=None, random_state=SEED)\n",
        "meta.fit(S_val, y_va)\n",
        "p_meta_val  = meta.predict_proba(S_val)[:,1].astype(np.float32)\n",
        "p_meta_test = meta.predict_proba(S_tst)[:,1].astype(np.float32)\n",
        "\n",
        "# Use meta output for final stage\n",
        "p_val, p_test = p_meta_val, p_meta_test"
      ],
      "metadata": {
        "id": "Q-9b5RJQNr3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- ACC overrides + per-bucket thresholds -----------------\n",
        "def vec_all_for_override(df):\n",
        "    def mk(tag):\n",
        "        rmap, cmap = pri[f\"rate{tag}\"], pri[f\"cnt{tag}\"]\n",
        "        vals = df[tag].values\n",
        "        r = np.array([rmap.get(v,gr) for v in vals], dtype=np.float32)\n",
        "        c = np.array([cmap.get(v,0)   for v in vals], dtype=np.int32)\n",
        "        return r,c\n",
        "    r_host,c_host=mk(\"_host\"); r_dom,c_dom=mk(\"_dom\"); r_tld,c_tld=mk(\"_tld\")\n",
        "    r_url,c_url  = mk(\"_url\");  r_pf,c_pf =mk(\"_pf\");  r_pl,c_pl =mk(\"_pl\"); r_qk,c_qk=mk(\"_qk\")\n",
        "    r_hpf,c_hpf  = mk(\"_h_pf\"); r_dpf,c_dpf=mk(\"_d_pf\"); r_dpl,c_dpl=mk(\"_d_pl\")\n",
        "    return (r_host,c_host,r_dom,c_dom,r_tld,c_tld,r_url,c_url,r_pf,c_pf,r_pl,c_pl,r_qk,c_qk,\n",
        "            r_hpf,c_hpf,r_dpf,c_dpf,r_dpl,c_dpl)\n",
        "\n",
        "(v_rh,v_ch,v_rd,v_cd,v_rt,v_ct,v_ru,v_cu,v_rpf,v_cpf,v_rpl,v_cpl,v_rqk,v_cqk,\n",
        " v_rhpf,v_chpf, v_rdpf,v_cdpf, v_rdpl,v_cdpl) = vec_all_for_override(val_df)\n",
        "(t_rh,t_ch,t_rd,t_cd,t_rt,t_ct,t_ru,t_cu,t_rpf,t_cpf,t_rpl,t_cpl,t_rqk,t_cqk,\n",
        " t_rhpf,t_chpf, t_rdpf,t_cdpf, t_rdpl,t_cdpl) = vec_all_for_override(test_df)\n",
        "\n",
        "thr_start, _ = best_acc_threshold(y_va, p_val)\n",
        "ov_best = {\"acc\":-1, \"thr\":thr_start}\n",
        "for mc in MIN_COUNT_LIST:\n",
        "    for plo in P_LO_LIST:\n",
        "        for phi in P_HI_LIST:\n",
        "            pv = apply_overrides(p_val,\n",
        "                                 v_ch,v_rh, v_cd,v_rd, v_ct,v_rt, v_cu,v_ru, v_cpf,v_rpf, v_cpl,v_rpl, v_cqk,v_rqk,\n",
        "                                 v_chpf,v_rhpf, v_cdpf,v_rdpf, v_cdpl,v_rdpl,\n",
        "                                 mc, plo, phi)\n",
        "            thr, acc = best_acc_threshold(y_va, pv)\n",
        "            if acc>ov_best[\"acc\"]:\n",
        "                ov_best={\"acc\":acc,\"thr\":thr,\"mc\":mc,\"plo\":plo,\"phi\":phi}\n",
        "\n",
        "p_val  = apply_overrides(p_val,\n",
        "                         v_ch,v_rh, v_cd,v_rd, v_ct,v_rt, v_cu,v_ru, v_cpf,v_rpf, v_cpl,v_rpl, v_cqk,v_rqk,\n",
        "                         v_chpf,v_rhpf, v_cdpf,v_rdpf, v_cdpl,v_rdpl,\n",
        "                         ov_best[\"mc\"], ov_best[\"plo\"], ov_best[\"phi\"])\n",
        "p_test = apply_overrides(p_test,\n",
        "                         t_ch,t_rh, t_cd,t_rd, t_ct,t_rt, t_cu,t_ru, t_cpf,t_rpf, t_cpl,t_rpl, t_rqk,t_rqk,\n",
        "                         t_chpf,t_rhpf, t_cdpf,t_rdpf, t_cdpl,t_rdpl,\n",
        "                         ov_best[\"mc\"], ov_best[\"plo\"], ov_best[\"phi\"])\n",
        "\n",
        "# per-bucket thresholds: host x domain x TLD counts\n",
        "vh = np.array([pri[\"cnt_host\"].get(h,0) for h in val_df[\"_host\"].values], dtype=np.int32)\n",
        "th = np.array([pri[\"cnt_host\"].get(h,0) for h in test_df[\"_host\"].values], dtype=np.int32)\n",
        "vd = np.array([pri[\"cnt_dom\"].get(d,0)  for d in val_df[\"_dom\"].values],  dtype=np.int32)\n",
        "td = np.array([pri[\"cnt_dom\"].get(d,0)  for d in test_df[\"_dom\"].values], dtype=np.int32)\n",
        "vt = np.array([pri[\"cnt_tld\"].get(t,0)  for t in val_df[\"_tld\"].values],  dtype=np.int32)\n",
        "tt = np.array([pri[\"cnt_tld\"].get(t,0)  for t in test_df[\"_tld\"].values], dtype=np.int32)\n",
        "\n",
        "vhb, thb = bucketize_vec(vh, HOST_BUCKETS), bucketize_vec(th, HOST_BUCKETS)\n",
        "vdb, tdb = bucketize_vec(vd, DOM_BUCKETS),  bucketize_vec(td, DOM_BUCKETS)\n",
        "vtb, ttb = bucketize_vec(vt, TLD_BUCKETS),  bucketize_vec(tt, TLD_BUCKETS)\n",
        "\n",
        "NHB, NDB, NTB = len(HOST_BUCKETS), len(DOM_BUCKETS), len(TLD_BUCKETS)\n",
        "val_combo  = (vhb*NDB + vdb)*NTB + vtb\n",
        "test_combo = (thb*NDB + tdb)*NTB + ttb\n",
        "NBUCKETS   = NHB * NDB * NTB\n",
        "\n",
        "bucket_thr = np.full(NBUCKETS, ov_best[\"thr\"], dtype=np.float32)\n",
        "for bi in range(NBUCKETS):\n",
        "    m = (val_combo==bi)\n",
        "    if m.sum() > 1:\n",
        "        thr_bi,_ = best_acc_threshold(y_va[m], p_val[m])\n",
        "        bucket_thr[bi]=thr_bi"
      ],
      "metadata": {
        "id": "_1skzliVNuft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Final TEST prediction & single-line metrics -----------------\n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_test = test_df[\"label\"].values\n",
        "yhat_test = np.zeros_like(y_test)\n",
        "for bi in range(NBUCKETS):\n",
        "    m = (test_combo==bi)\n",
        "    if m.sum(): yhat_test[m] = (p_test[m] >= bucket_thr[bi]).astype(int)\n",
        "\n",
        "acc = accuracy_score(y_test, yhat_test)\n",
        "rec = recall_score(y_test, yhat_test, zero_division=0)\n",
        "f1  = f1_score(y_test, yhat_test, zero_division=0)\n",
        "try: auc = roc_auc_score(y_test, p_test)\n",
        "except: auc = float(\"nan\")\n",
        "\n",
        "print(f\"FINAL_TEST_METRICS acc={acc:.6f} auc={auc:.6f} f1={f1:.6f} rec={rec:.6f}\")\n",
        "ttotal()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-83JPxXNxrZ",
        "outputId": "c84161bc-d5cd-4ca8-eae9-5fa02c7368f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL_TEST_METRICS acc=0.971693 auc=0.996113 f1=0.971573 rec=0.967462\n",
            "[TIMER] TOTAL: 266.8s\n"
          ]
        }
      ]
    }
  ]
}